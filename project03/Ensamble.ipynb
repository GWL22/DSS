{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_train = dd.read_csv('dataset/train.csv', parse_dates=['srch_ci', 'srch_co'])\n",
    "train = whole_train.get_partition(0).head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\pandas\\core\\indexing.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "train['nights'] = train['srch_co'] - train['srch_ci']\n",
    "train['nights'] = train['nights'] / np.timedelta64(1, 'D')\n",
    "\n",
    "for num in range(len(train['nights'])):\n",
    "    if math.isnan(train['nights'].ix[num]) or train['nights'].ix[num]<0:\n",
    "        num1 = num - 1\n",
    "        num2 = num + 1\n",
    "        if train.ix[num]['user_id'] == train.ix[num]['user_id'] and math.isnan(train['nights'].ix[num1]) == False:\n",
    "            train['nights'].ix[num] = train.ix[num1]['nights']\n",
    "        elif train.ix[num]['user_id'] == train.ix[num]['user_id'] and math.isnan(train['nights'].ix[num2]) == False:\n",
    "            train['nights'].ix[num] = train.ix[num2]['nights']\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train['srch_ci']\n",
    "del train['srch_co']\n",
    "del train['date_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train['site_name']\n",
    "del train['user_id']\n",
    "del train['orig_destination_distance']\n",
    "# del train['user_location_city']\n",
    "# del train['user_location_region']\n",
    "# del train['user_location_country']\n",
    "# del train['posa_continent']\n",
    "\n",
    "# del train['is_package']\n",
    "# del train['is_mobile']\n",
    "# del train['channel']\n",
    "\n",
    "# del train['srch_adults_cnt']\n",
    "# del train['srch_children_cnt']\n",
    "# del train['srch_rm_cnt']\n",
    "\n",
    "# del train['srch_destination_id']\n",
    "# del train['srch_destination_type_id']\n",
    "# del train['is_booking']\n",
    "# del train['cnt']\n",
    "\n",
    "# del train['hotel_continent']\n",
    "# del train['hotel_country']\n",
    "# del train['hotel_market']\n",
    "# del train['nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = train.drop('hotel_cluster', axis=1)\n",
    "y = train['hotel_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'posa_continent', u'user_location_country', u'user_location_region',\n",
      "       u'user_location_city', u'is_mobile', u'is_package', u'channel',\n",
      "       u'srch_adults_cnt', u'srch_children_cnt', u'srch_rm_cnt',\n",
      "       u'srch_destination_id', u'srch_destination_type_id', u'is_booking',\n",
      "       u'cnt', u'hotel_continent', u'hotel_country', u'hotel_market',\n",
      "       u'nights'],\n",
      "      dtype='object')\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print x.columns\n",
    "print len(x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('srch_rm_cnt', 'is_mobile', 'posa_continent', 'is_package')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.columns[9], x.columns[4], x.columns[0], x.columns[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature ranking:\n",
      "1. feature 13 (0.152734275854)\n",
      "2. feature 16 (0.11365034211)\n",
      "3. feature 10 (0.110828570645)\n",
      "4. feature 3 (0.0942675926922)\n",
      "5. feature 17 (0.0931406983621)\n",
      "6. feature 2 (0.0741986000355)\n",
      "7. feature 6 (0.063355967503)\n",
      "8. feature 7 (0.047362370705)\n",
      "9. feature 15 (0.0439833966729)\n",
      "10. feature 11 (0.0392254306781)\n",
      "11. feature 8 (0.0311360807798)\n",
      "12. feature 1 (0.0287064750128)\n",
      "13. feature 12 (0.0274025682738)\n",
      "14. feature 14 (0.0272793063565)\n",
      "15. feature 9 (0.0143315606569)\n",
      "16. feature 4 (0.0134264366504)\n",
      "17. feature 0 (0.0130316836859)\n",
      "18. feature 5 (0.0119386433262)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFyCAYAAAB/b0lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBEmSVMsgQZIk1TJIkCRJtf5/Yt7ZI+Te4q4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13f87570>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import clone\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "forest = ExtraTreesClassifier(n_estimators=18, random_state=0)\n",
    "forest.fit(x, y)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis = 0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print 'feature ranking:'\n",
    "for f in range(x.shape[1]):\n",
    "    print '{}. feature {} ({})'.format(f+1, indices[f], importances[indices[f]])\n",
    "\n",
    "\n",
    "plt.title('Feature importances')\n",
    "plt.bar(range(x.shape[1]), importances[indices], color='r', yerr=std[indices], align='center')\n",
    "plt.xticks(range(x.shape[1]), indices)\n",
    "plt.xlim([-1, x.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'srch_children_cnt'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.columns[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "model1 = DecisionTreeClassifier().fit(x,y)\n",
    "model2 = BaggingClassifier(DecisionTreeClassifier(), bootstrap_features=True, random_state=0).fit(x,y)\n",
    "\n",
    "result1 = model1.predict(x)\n",
    "result2 = model2.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.91      0.64        98\n",
      "          1       0.66      0.95      0.78       112\n",
      "          2       0.52      0.92      0.67        78\n",
      "          3       0.56      0.94      0.71        51\n",
      "          4       0.62      0.79      0.70        77\n",
      "          5       0.52      0.77      0.62       120\n",
      "          6       0.67      0.78      0.72       118\n",
      "          7       0.58      0.91      0.71        86\n",
      "          8       0.52      0.79      0.63       137\n",
      "          9       0.51      0.81      0.63        94\n",
      "         10       0.51      0.84      0.64       103\n",
      "         11       0.59      0.76      0.67        68\n",
      "         12       0.68      0.88      0.77       138\n",
      "         13       0.61      0.82      0.70        90\n",
      "         14       0.61      0.71      0.66        84\n",
      "         15       0.69      0.78      0.73        87\n",
      "         16       0.69      0.76      0.72       144\n",
      "         17       0.56      0.68      0.61        74\n",
      "         18       0.63      0.69      0.66       160\n",
      "         19       0.74      0.70      0.72        66\n",
      "         20       0.54      0.74      0.62        92\n",
      "         21       0.71      0.74      0.73       156\n",
      "         22       0.54      0.74      0.62        88\n",
      "         23       0.76      0.82      0.79        77\n",
      "         24       0.66      0.80      0.72        44\n",
      "         25       0.75      0.82      0.79       188\n",
      "         26       0.69      0.83      0.75        83\n",
      "         27       0.89      1.00      0.94        16\n",
      "         28       0.76      0.78      0.77       134\n",
      "         29       0.60      0.72      0.66        89\n",
      "         30       0.61      0.58      0.60       129\n",
      "         31       0.62      0.72      0.67        74\n",
      "         32       0.68      0.74      0.71        85\n",
      "         33       0.73      0.82      0.77       181\n",
      "         34       0.67      0.66      0.67        92\n",
      "         35       0.59      0.63      0.61        68\n",
      "         36       0.60      0.83      0.70       135\n",
      "         37       0.68      0.57      0.62       112\n",
      "         38       0.52      0.69      0.59        72\n",
      "         39       0.61      0.73      0.66        99\n",
      "         40       0.69      0.75      0.72       142\n",
      "         41       0.51      0.72      0.60       237\n",
      "         42       0.75      0.66      0.71       125\n",
      "         43       0.82      0.59      0.69        69\n",
      "         44       0.73      0.73      0.73        55\n",
      "         45       0.63      0.76      0.69        63\n",
      "         46       0.83      0.71      0.76        96\n",
      "         47       0.74      0.60      0.67       106\n",
      "         48       0.79      0.74      0.76       204\n",
      "         49       0.76      0.68      0.72        41\n",
      "         50       0.85      0.60      0.70       128\n",
      "         51       0.83      0.59      0.69        98\n",
      "         52       0.66      0.74      0.70       132\n",
      "         53       0.90      0.72      0.80        53\n",
      "         54       0.70      0.79      0.74        82\n",
      "         55       0.64      0.55      0.59        75\n",
      "         56       0.68      0.75      0.71       104\n",
      "         57       0.71      0.64      0.67       133\n",
      "         58       0.78      0.70      0.74       105\n",
      "         59       0.82      0.54      0.65        90\n",
      "         60       0.88      0.64      0.74        88\n",
      "         61       0.81      0.55      0.65       110\n",
      "         62       0.71      0.59      0.64        94\n",
      "         63       0.78      0.75      0.76        83\n",
      "         64       0.78      0.76      0.77       127\n",
      "         65       0.69      0.86      0.76       227\n",
      "         66       1.00      0.57      0.73        58\n",
      "         67       0.82      0.70      0.75        57\n",
      "         68       0.85      0.70      0.76       112\n",
      "         69       0.85      0.59      0.69        75\n",
      "         70       0.79      0.66      0.72       154\n",
      "         71       0.65      0.67      0.66        51\n",
      "         72       0.86      0.53      0.66       118\n",
      "         73       0.83      0.61      0.70       115\n",
      "         74       1.00      0.89      0.94         9\n",
      "         75       0.81      0.53      0.64        32\n",
      "         76       0.83      0.56      0.67       126\n",
      "         77       0.94      0.58      0.72       112\n",
      "         78       0.67      0.48      0.56       100\n",
      "         79       0.84      0.42      0.56        62\n",
      "         80       0.81      0.78      0.79        49\n",
      "         81       0.96      0.56      0.71        96\n",
      "         82       0.89      0.69      0.78       101\n",
      "         83       0.87      0.56      0.68       142\n",
      "         84       0.93      0.47      0.63        91\n",
      "         85       0.68      0.47      0.55        92\n",
      "         86       0.83      0.72      0.77        60\n",
      "         87       0.80      0.54      0.64        67\n",
      "         88       1.00      0.62      0.77        29\n",
      "         89       0.80      0.59      0.68        74\n",
      "         90       1.00      0.53      0.69        83\n",
      "         91       0.78      0.66      0.72       257\n",
      "         92       0.88      0.79      0.84        58\n",
      "         93       1.00      0.32      0.49        37\n",
      "         94       0.66      0.52      0.58        63\n",
      "         95       0.90      0.66      0.76       163\n",
      "         96       0.97      0.58      0.73       128\n",
      "         97       0.79      0.69      0.74       133\n",
      "         98       0.86      0.58      0.69       105\n",
      "         99       0.84      0.55      0.67       125\n",
      "\n",
      "avg / total       0.73      0.70      0.70     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y, result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6957\n"
     ]
    }
   ],
   "source": [
    "print accuracy_score(y, result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "\n",
    "model_1 = DecisionTreeClassifier().fit(x,y)\n",
    "model_2 = BaggingClassifier().fit(x,y)\n",
    "model_3 = SVC(probability=True).fit(x,y)\n",
    "model_4 = VotingClassifier(estimators=[('dt', model_1), ('Bg', model_2), ('svc', model_3)],\n",
    "                           voting='soft', weights=[3,1,0]).fit(x,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.63      0.58        98\n",
      "          1       0.57      0.89      0.69       112\n",
      "          2       0.58      0.55      0.57        78\n",
      "          3       0.95      0.37      0.54        51\n",
      "          4       0.73      0.43      0.54        77\n",
      "          5       0.49      0.52      0.50       120\n",
      "          6       0.58      0.54      0.56       118\n",
      "          7       0.73      0.60      0.66        86\n",
      "          8       0.40      0.66      0.50       137\n",
      "          9       0.54      0.55      0.55        94\n",
      "         10       0.58      0.50      0.54       103\n",
      "         11       0.73      0.40      0.51        68\n",
      "         12       0.64      0.67      0.65       138\n",
      "         13       0.65      0.62      0.64        90\n",
      "         14       0.82      0.38      0.52        84\n",
      "         15       0.61      0.43      0.50        87\n",
      "         16       0.54      0.61      0.57       144\n",
      "         17       0.65      0.30      0.41        74\n",
      "         18       0.56      0.54      0.55       160\n",
      "         19       0.78      0.48      0.60        66\n",
      "         20       0.62      0.43      0.51        92\n",
      "         21       0.55      0.61      0.58       156\n",
      "         22       0.57      0.52      0.54        88\n",
      "         23       0.85      0.53      0.66        77\n",
      "         24       0.64      0.61      0.63        44\n",
      "         25       0.59      0.79      0.67       188\n",
      "         26       0.81      0.55      0.66        83\n",
      "         27       0.88      0.94      0.91        16\n",
      "         28       0.70      0.66      0.68       134\n",
      "         29       0.51      0.46      0.49        89\n",
      "         30       0.48      0.51      0.49       129\n",
      "         31       0.65      0.46      0.54        74\n",
      "         32       0.63      0.54      0.58        85\n",
      "         33       0.62      0.78      0.69       181\n",
      "         34       0.54      0.55      0.55        92\n",
      "         35       0.54      0.41      0.47        68\n",
      "         36       0.48      0.74      0.58       135\n",
      "         37       0.56      0.39      0.46       112\n",
      "         38       0.60      0.44      0.51        72\n",
      "         39       0.52      0.63      0.57        99\n",
      "         40       0.54      0.65      0.59       142\n",
      "         41       0.31      0.63      0.42       237\n",
      "         42       0.59      0.61      0.60       125\n",
      "         43       0.70      0.46      0.56        69\n",
      "         44       0.93      0.47      0.63        55\n",
      "         45       0.63      0.70      0.66        63\n",
      "         46       0.58      0.67      0.62        96\n",
      "         47       0.63      0.45      0.53       106\n",
      "         48       0.63      0.71      0.67       204\n",
      "         49       0.81      0.41      0.55        41\n",
      "         50       0.53      0.66      0.59       128\n",
      "         51       0.71      0.43      0.54        98\n",
      "         52       0.66      0.48      0.55       132\n",
      "         53       0.84      0.60      0.70        53\n",
      "         54       0.62      0.78      0.69        82\n",
      "         55       0.46      0.31      0.37        75\n",
      "         56       0.54      0.62      0.58       104\n",
      "         57       0.61      0.56      0.58       133\n",
      "         58       0.61      0.63      0.62       105\n",
      "         59       0.69      0.37      0.48        90\n",
      "         60       0.75      0.53      0.62        88\n",
      "         61       0.50      0.51      0.51       110\n",
      "         62       0.54      0.50      0.52        94\n",
      "         63       0.57      0.63      0.60        83\n",
      "         64       0.66      0.65      0.65       127\n",
      "         65       0.52      0.93      0.67       227\n",
      "         66       0.92      0.40      0.55        58\n",
      "         67       0.69      0.51      0.59        57\n",
      "         68       0.61      0.56      0.58       112\n",
      "         69       0.67      0.43      0.52        75\n",
      "         70       0.62      0.68      0.65       154\n",
      "         71       0.39      0.45      0.42        51\n",
      "         72       0.56      0.42      0.48       118\n",
      "         73       0.56      0.60      0.58       115\n",
      "         74       1.00      0.89      0.94         9\n",
      "         75       0.88      0.44      0.58        32\n",
      "         76       0.52      0.53      0.53       126\n",
      "         77       0.75      0.52      0.61       112\n",
      "         78       0.43      0.43      0.43       100\n",
      "         79       0.49      0.34      0.40        62\n",
      "         80       0.69      0.73      0.71        49\n",
      "         81       0.63      0.47      0.54        96\n",
      "         82       0.72      0.62      0.67       101\n",
      "         83       0.55      0.59      0.57       142\n",
      "         84       0.62      0.38      0.48        91\n",
      "         85       0.53      0.51      0.52        92\n",
      "         86       0.82      0.45      0.58        60\n",
      "         87       0.73      0.36      0.48        67\n",
      "         88       0.86      0.41      0.56        29\n",
      "         89       0.52      0.46      0.49        74\n",
      "         90       0.74      0.42      0.54        83\n",
      "         91       0.53      0.75      0.62       257\n",
      "         92       0.82      0.69      0.75        58\n",
      "         93       1.00      0.22      0.36        37\n",
      "         94       0.57      0.38      0.46        63\n",
      "         95       0.65      0.67      0.66       163\n",
      "         96       0.57      0.57      0.57       128\n",
      "         97       0.59      0.74      0.66       133\n",
      "         98       0.55      0.57      0.56       105\n",
      "         99       0.50      0.59      0.54       125\n",
      "\n",
      "avg / total       0.60      0.58      0.57     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y, model_3.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5753\n"
     ]
    }
   ],
   "source": [
    "print accuracy_score(y, model_3.predict(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* VotingClassifier(DecisionTreeClassifier, BaggingClassifier, SVC, weight: 2,1,1) \n",
    "    * acc : 0.6955 precision : 0.71 recall : 0.70\n",
    "* DecisionClassifier \n",
    "    * acc: 0.6957 precision: 0.73 recall: 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(),\n",
    "                         algorithm='SAMME',\n",
    "                         n_estimators=18)\n",
    "\n",
    "model_5 = bdt.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.76      0.69        98\n",
      "          1       0.69      0.88      0.78       112\n",
      "          2       0.66      0.81      0.73        78\n",
      "          3       0.65      0.86      0.74        51\n",
      "          4       0.72      0.70      0.71        77\n",
      "          5       0.61      0.64      0.62       120\n",
      "          6       0.73      0.74      0.73       118\n",
      "          7       0.66      0.83      0.73        86\n",
      "          8       0.57      0.69      0.63       137\n",
      "          9       0.58      0.72      0.64        94\n",
      "         10       0.64      0.70      0.67       103\n",
      "         11       0.67      0.69      0.68        68\n",
      "         12       0.65      0.88      0.75       138\n",
      "         13       0.61      0.79      0.69        90\n",
      "         14       0.68      0.67      0.67        84\n",
      "         15       0.69      0.78      0.73        87\n",
      "         16       0.71      0.75      0.73       144\n",
      "         17       0.53      0.68      0.60        74\n",
      "         18       0.69      0.65      0.67       160\n",
      "         19       0.76      0.68      0.72        66\n",
      "         20       0.64      0.66      0.65        92\n",
      "         21       0.64      0.76      0.70       156\n",
      "         22       0.75      0.59      0.66        88\n",
      "         23       0.78      0.81      0.79        77\n",
      "         24       0.62      0.84      0.71        44\n",
      "         25       0.74      0.83      0.78       188\n",
      "         26       0.80      0.80      0.80        83\n",
      "         27       0.89      1.00      0.94        16\n",
      "         28       0.75      0.78      0.76       134\n",
      "         29       0.61      0.69      0.65        89\n",
      "         30       0.55      0.60      0.57       129\n",
      "         31       0.61      0.69      0.65        74\n",
      "         32       0.74      0.73      0.73        85\n",
      "         33       0.79      0.80      0.79       181\n",
      "         34       0.72      0.62      0.67        92\n",
      "         35       0.67      0.53      0.59        68\n",
      "         36       0.63      0.80      0.71       135\n",
      "         37       0.57      0.62      0.60       112\n",
      "         38       0.61      0.60      0.61        72\n",
      "         39       0.62      0.69      0.65        99\n",
      "         40       0.69      0.72      0.70       142\n",
      "         41       0.49      0.67      0.57       237\n",
      "         42       0.72      0.67      0.70       125\n",
      "         43       0.45      0.71      0.55        69\n",
      "         44       0.79      0.69      0.74        55\n",
      "         45       0.65      0.71      0.68        63\n",
      "         46       0.77      0.73      0.75        96\n",
      "         47       0.72      0.59      0.65       106\n",
      "         48       0.71      0.74      0.72       204\n",
      "         49       0.81      0.63      0.71        41\n",
      "         50       0.73      0.66      0.69       128\n",
      "         51       0.73      0.62      0.67        98\n",
      "         52       0.71      0.69      0.70       132\n",
      "         53       0.79      0.77      0.78        53\n",
      "         54       0.79      0.71      0.75        82\n",
      "         55       0.56      0.59      0.57        75\n",
      "         56       0.61      0.78      0.68       104\n",
      "         57       0.68      0.63      0.65       133\n",
      "         58       0.69      0.72      0.71       105\n",
      "         59       0.73      0.57      0.64        90\n",
      "         60       0.64      0.75      0.69        88\n",
      "         61       0.68      0.59      0.63       110\n",
      "         62       0.59      0.60      0.59        94\n",
      "         63       0.82      0.70      0.75        83\n",
      "         64       0.77      0.75      0.76       127\n",
      "         65       0.67      0.89      0.76       227\n",
      "         66       0.85      0.60      0.71        58\n",
      "         67       0.82      0.70      0.75        57\n",
      "         68       0.75      0.71      0.73       112\n",
      "         69       0.79      0.60      0.68        75\n",
      "         70       0.66      0.64      0.65       154\n",
      "         71       0.62      0.69      0.65        51\n",
      "         72       0.52      0.62      0.57       118\n",
      "         73       0.78      0.63      0.70       115\n",
      "         74       1.00      0.89      0.94         9\n",
      "         75       0.72      0.56      0.63        32\n",
      "         76       0.78      0.58      0.66       126\n",
      "         77       0.78      0.62      0.69       112\n",
      "         78       0.59      0.51      0.55       100\n",
      "         79       0.60      0.45      0.51        62\n",
      "         80       0.75      0.80      0.77        49\n",
      "         81       0.62      0.64      0.63        96\n",
      "         82       0.80      0.69      0.74       101\n",
      "         83       0.66      0.58      0.62       142\n",
      "         84       0.82      0.49      0.62        91\n",
      "         85       0.75      0.42      0.54        92\n",
      "         86       0.83      0.72      0.77        60\n",
      "         87       0.85      0.49      0.62        67\n",
      "         88       0.83      0.69      0.75        29\n",
      "         89       0.77      0.58      0.66        74\n",
      "         90       0.66      0.59      0.62        83\n",
      "         91       0.73      0.67      0.70       257\n",
      "         92       0.80      0.83      0.81        58\n",
      "         93       1.00      0.32      0.49        37\n",
      "         94       0.63      0.49      0.55        63\n",
      "         95       0.89      0.66      0.76       163\n",
      "         96       0.77      0.60      0.68       128\n",
      "         97       0.77      0.69      0.73       133\n",
      "         98       0.92      0.54      0.68       105\n",
      "         99       0.71      0.58      0.64       125\n",
      "\n",
      "avg / total       0.70      0.68      0.68     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y, model_5.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0207\n"
     ]
    }
   ],
   "source": [
    "print accuracy_score(model_6.predict(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model_6 = GradientBoostingClassifier(n_estimators=18, learning_rate=1.0, random_state=0).fit(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
